{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集称为“二十新闻组”。这是官方的说明，从网站引用：\n",
    "#### 20个新闻组数据集是大约20,000个新闻组文档的集合，在20个不同的新闻组中被平均分配（几乎）。\n",
    "#### 据我们所知，它最初是由肯朗收集的，可能是他的论文“Newsweeder：学习过滤网络新闻”，虽然他没有明确提及这个集合。\n",
    "#### 20个新闻组集合已成为机器学习技术文本应用中的实验的流行数据集，如文本分类和文本聚类。\n",
    "#### 在下面我们将使用内置的数据集加载器来从scikit学习的20个新闻组。\n",
    "#### 或者，可以从网站手动下载数据集，并将该sklearn.datasets.load_files 功能指向20news-bydate-train未压缩归档文件夹的子文件夹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# 获取数据集中的20个可用数据集中仅使用4个类别的部分数据集：\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "# 我们现在可以加载与这些类别匹配的文件列表，如下所示：\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "twenty_train.data: 2257\n",
      "twenty_train.filenames 2257\n",
      "[ 'C:\\\\Users\\\\lenovo\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38440'\n",
      " 'C:\\\\Users\\\\lenovo\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38479'\n",
      " 'C:\\\\Users\\\\lenovo\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\soc.religion.christian\\\\20737'\n",
      " ...,\n",
      " 'C:\\\\Users\\\\lenovo\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.med\\\\58112'\n",
      " 'C:\\\\Users\\\\lenovo\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.med\\\\58578'\n",
      " 'C:\\\\Users\\\\lenovo\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.med\\\\58895']\n"
     ]
    }
   ],
   "source": [
    "# 返回的数据集是一个scikit-learn“束”：一个简单的持有者对象，其字段可以作为python dict 键或object属性访问，\n",
    "# 以方便起见，例如 target_names保存所请求的类别名称的列表：\n",
    "print(twenty_train.target_names)\n",
    "# 文件本身被加载到data属性的内存中。作为参考，文件名也可用：\n",
    "print(\"twenty_train.data:\",len(twenty_train.data))\n",
    "print(\"twenty_train.filenames\",len(twenty_train.filenames))\n",
    "print(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "\n",
      " comp.graphics\n",
      "[1 1 3 3 3 3 3 2 2 2]\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "# 我们打印第一个加载的文件的第一行：\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "# 受监督的学习算法将需要训练集中的每个文档的类别标签。在这种情况下，类别是新闻组的名称，它也恰好是保存单个文档的文件夹的名称。\n",
    "# 对于速度和空间效率，scikit-learn将目标属性加载为与target_names列表中类别名称的索引对应的整数数组。每个样本的类别整数ID存储在target属性中：\n",
    "print(\"\\n\",twenty_train.target_names[twenty_train.target[0]])\n",
    "print(twenty_train.target[:10])\n",
    "# 可以如下回到类别名称：\n",
    "for t in twenty_train.target[:10]:\n",
    "     print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用表征文本scikit-learn\n",
    "\n",
    "#### 文本预处理，令牌化和过滤的无效词包含在高级组件中，能够构建特征字典并将文档转换为特征向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print(X_train_counts.shape)\n",
    "# CountVectorizer支持N克单词或连续字符的计数。一旦装配，矢量化程序已经建立了一个特征索引字典：\n",
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从出现到频率\n",
    "\n",
    "#### 发生次数是一个很好的开始，但是有一个问题：更长的文档将具有比较短的文档更高的平均值，即使他们可能会讨论相同的主题。\n",
    "#### 为了避免这些潜在差异，将文档中每个单词的出现次数除以文档中的单词总数就足够了：这些新功能被称为tf“术语频率”。\n",
    "#### 在tf之上的另一个改进是对在语料库中的许多文档中发生的词的缩减权重，因此比仅在较小部分语料库中出现的词语的信息量更少。\n",
    "#### 这个缩减被称为tf-idf，用于“Term Frequency times Inverse Document Frequency”。\n",
    "#### 两个TF和TF-IDF可以计算如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n",
      "(2257, 35788)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf.shape)\n",
    "\n",
    "# 在上面的示例代码中，我们首先使用该fit(..)方法将我们的估计器与数据进行匹配，\n",
    "#其次是transform(..)将我们的计数矩阵转换为tf-idf表示法的方法。\n",
    "#通过跳过冗余处理，可以组合这两个步骤，以更快地实现相同的最终结果。\n",
    "#这是通过使用fit_transform(..)如下所示的方法完成的 ，如上一节中的注释所述：\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练分类器\n",
    "\n",
    "#### 现在我们有了我们的功能，我们可以训练分类器来尝试预测一个帖子的类别。\n",
    "#### 让我们从一个天真的贝叶斯 分类器开始，为这个任务提供了一个很好的基准。\n",
    "#### scikit-learn包括该分类器的几个变体; 最适合单词计数的是多项式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "# 为了尝试在新文档中预测结果，我们需要使用与以前几乎相同的特征提取链来提取特征。\n",
    "# 不同的是，我们所说transform的，而不是fit_transform 对变压器，因为它们已经适应训练集：\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建管道\n",
    "#### 为了使vectorizer => transform => classifier更容易使用，scikit-learn提供了一个Pipeline类似复合分类器的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集上的性能评估\n",
    "#### 评估模型的预测准确度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83488681757656458"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "     categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 让我们看看我们是否可以用线性支持向量机（SVM）来做得更好，\n",
    "#### 这是被广泛认为是最好的文本分类算法之一（尽管它也比初学贝叶斯慢一些）。\n",
    "#### 我们可以通过将不同的分类对象插入到我们的管道中来改变学习者："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9127829560585885"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=5, tol=None)),])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进一步提供实用程序以更详细地对结果进行性能分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[258,  11,  15,  35],\n",
       "       [  4, 379,   3,   3],\n",
       "       [  5,  33, 355,   3],\n",
       "       [  5,  10,   4, 379]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,target_names=twenty_test.target_names))\n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数整定使用格搜索\n",
    "#### 我们已经遇到了一些参数，如use_idf在 TfidfTransformer。分类器也有许多参数; 例如，MultinomialNB包括平滑参数alpha， 并且在目标函数中SGDClassifier具有惩罚参数alpha和可配置的损失和惩罚项（参见模块文档或使用Python help函数来获取这些描述）。\n",
    "#### 代替调整链的各种组件的参数，可以对可能值的网格上的最佳参数进行详尽的搜索。我们尝试使用单词或双字母的所有分类器，具有或不具有idf，对于线性SVM，可以使用0.01或0.001的惩罚参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n",
      "{'mean_train_score': array([ 0.99374372,  1.        ,  0.94123886,  0.97623272,  1.        ,\n",
      "        1.        ,  0.98499057,  1.        ]), 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2) (1, 1) (1, 2)],\n",
      "             mask = [False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'mean_score_time': array([ 0.0496695 ,  0.11267312,  0.09800577,  0.21101205,  0.06167014,\n",
      "        0.12067358,  0.07633766,  0.11934018]), 'std_train_score': array([ 0.00355103,  0.        ,  0.00779216,  0.00989579,  0.        ,\n",
      "        0.        ,  0.01061333,  0.        ]), 'rank_test_score': array([3, 4, 8, 6, 1, 2, 7, 5]), 'std_fit_time': array([ 0.01892782,  0.08209399,  0.20419503,  0.10186309,  0.02758173,\n",
      "        0.3004017 ,  0.34428207,  0.01564272]), 'split2_test_score': array([ 0.89473684,  0.90225564,  0.81203008,  0.81203008,  0.93984962,\n",
      "        0.92481203,  0.82706767,  0.86466165]), 'std_test_score': array([ 0.01500217,  0.02847571,  0.05120452,  0.05605779,  0.03082125,\n",
      "        0.0254174 ,  0.04620275,  0.04244373]), 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False],\n",
      "             mask = [False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_clf__alpha': masked_array(data = [0.01 0.01 0.01 0.01 0.001 0.001 0.001 0.001],\n",
      "             mask = [False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split1_train_score': array([ 0.99625468,  1.        ,  0.95131086,  0.98501873,  1.        ,\n",
      "        1.        ,  0.97752809,  1.        ]), 'params': [{'tfidf__use_idf': True, 'clf__alpha': 0.01, 'vect__ngram_range': (1, 1)}, {'tfidf__use_idf': True, 'clf__alpha': 0.01, 'vect__ngram_range': (1, 2)}, {'tfidf__use_idf': False, 'clf__alpha': 0.01, 'vect__ngram_range': (1, 1)}, {'tfidf__use_idf': False, 'clf__alpha': 0.01, 'vect__ngram_range': (1, 2)}, {'tfidf__use_idf': True, 'clf__alpha': 0.001, 'vect__ngram_range': (1, 1)}, {'tfidf__use_idf': True, 'clf__alpha': 0.001, 'vect__ngram_range': (1, 2)}, {'tfidf__use_idf': False, 'clf__alpha': 0.001, 'vect__ngram_range': (1, 1)}, {'tfidf__use_idf': False, 'clf__alpha': 0.001, 'vect__ngram_range': (1, 2)}], 'split2_train_score': array([ 0.99625468,  1.        ,  0.94007491,  0.98127341,  1.        ,\n",
      "        1.        ,  1.        ,  1.        ]), 'std_score_time': array([ 0.0103715 ,  0.01407999,  0.03182433,  0.08033313,  0.00249455,\n",
      "        0.00939325,  0.01602873,  0.00555808]), 'mean_test_score': array([ 0.8775,  0.875 ,  0.765 ,  0.78  ,  0.9   ,  0.89  ,  0.7675,  0.81  ]), 'split0_test_score': array([ 0.85820896,  0.8358209 ,  0.69402985,  0.70149254,  0.89552239,\n",
      "        0.88059701,  0.76119403,  0.76119403]), 'mean_fit_time': array([ 0.13167413,  0.51502943,  0.46869342,  1.24740458,  0.38902227,\n",
      "        0.96538854,  0.5040288 ,  0.73004174]), 'split0_train_score': array([ 0.9887218 ,  1.        ,  0.93233083,  0.96240602,  1.        ,\n",
      "        1.        ,  0.97744361,  1.        ]), 'split1_test_score': array([ 0.87969925,  0.88721805,  0.78947368,  0.82706767,  0.86466165,\n",
      "        0.86466165,  0.71428571,  0.80451128])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),}\n",
    "# 显然，这种详尽的搜索可能是昂贵的。如果我们拥有多个CPU内核，我们可以告诉电网搜索者与参数并行地尝试这八个参数组合n_jobs。\n",
    "# 如果我们给这个参数一个值-1，网格搜索将检测安装了多少个核心并使用它们全部：\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "#print(gs_clf)\n",
    "# 网格搜索实例的行为与普通scikit-learn 模型类似。让我们在训练数据的较小子集上进行搜索，以加速计算：\n",
    "gs_clf= gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n",
    "#print(gs_clf)\n",
    "# 调用的结果fit一上GridSearchCV对象的分类，我们可以用它来predict：\n",
    "print(twenty_train.target_names[gs_clf.predict(['God is love'])[0]])\n",
    "# 对象best_score_和best_params_属性存储与该分数相对应的最佳平均分数和参数设置：\n",
    "gs_clf.best_score_                                  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "print(gs_clf.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
